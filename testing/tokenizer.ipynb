{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom implementation of a BPE Tokenizer from scratch\n",
    "\n",
    "References:\n",
    "1. GPT Tokenizer by Andrej Karpathy: https://www.youtube.com/watch?v=zduSFxRajkE\n",
    "2. Byte pair encoding on Wikipedia: https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode code point of ðŸ˜‚: 128514\n",
      "UTF-8 representation of ðŸ˜‚: b'\\xf0\\x9f\\x98\\x82'\n",
      "Byte encoded form of ðŸ˜‚: [240, 159, 152, 130]\n"
     ]
    }
   ],
   "source": [
    "char = 'ðŸ˜‚'\n",
    "enc = char.encode('utf-8')\n",
    "print(f\"Unicode code point of {char}: {ord(char)}\")\n",
    "print(f\"UTF-8 representation of {char}: {enc}\")\n",
    "print(f\"Byte encoded form of {char}: {list(map(int, enc))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = \"hello world!\"\n",
    "def encode(str):\n",
    "    return list(map(int, str.encode('utf-8')))\n",
    "\n",
    "# Byte encoded form of words\n",
    "encode(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(73, 39): 1, (39, 109): 1, (109, 32): 2, (32, 116): 4, (116, 104): 2, (104, 105): 1, (105, 110): 3, (110, 107): 1, (107, 105): 1, (110, 103): 2, (103, 32): 2, (32, 111): 1, (111, 102): 1, (102, 32): 1, (32, 103): 2, (103, 111): 1, (111, 105): 1, (116, 111): 2, (111, 32): 1, (104, 101): 1, (101, 32): 1, (103, 121): 1, (121, 109): 1, (111, 100): 1, (100, 97): 1, (97, 121): 1, (121, 33): 1}\n",
      "Top pair: (32, 116)\n"
     ]
    }
   ],
   "source": [
    "def get_pairs(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens[0:], tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "    \n",
    "sen = \"I'm thinking of going to the gym today!\"\n",
    "stats = get_pairs(encode(sen))\n",
    "top_pair = max(stats, key=stats.get)    # pair with max freq\n",
    "ordered = list(sorted(stats.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "print(stats)\n",
    "print(f\"Top pair: {top_pair}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 99]\n",
      "[1, 2, 3, 100]\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # in the list ids look for matching pairs and replace it with idx\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "num = [1, 2, 3, 4, 5, 6]\n",
    "m1 = merge(num, (5, 6), 99)    # merge1: (5, 6) -> 99\n",
    "m2 = merge(m1, (4, 99), 100)   # merge2: (4, 99) -> 100\n",
    "print(m1)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top pair in the tokens is: (101, 32)\n",
      "Length of tokens before merging: 1054\n",
      "Length of tokens after merging: 1027\n"
     ]
    }
   ],
   "source": [
    "text = \"Byte pair encoding[1][2] (also known as digram coding)[3] is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling.[4] Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words).[5][6][7] This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial \\\"tokens\\\"). Then, successively, the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained. Note that new words can always be constructed from final vocabulary tokens and initial-set characters.[8] This algorithmic approach has been extended from spoken language to sign language in recent years.[9]\"\n",
    "tokens = text.encode('utf-8')\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "# Tokens before merging\n",
    "pairs = get_pairs(tokens)\n",
    "top_pair = max(pairs, key=pairs.get)\n",
    "print(F\"Top pair in the tokens is: {top_pair}\")\n",
    "print(f\"Length of tokens before merging: {len(tokens)}\")\n",
    "\n",
    "# Tokens after merging\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(f\"Length of tokens after merging: {len(tokens2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base vocabulary typically includes individual characters and special tokens. In many implementations, this base set is assumed to be 256 tokens, corresponding to the standard ASCII character set. Therefore, num_merges is calculated as vocab_size - 256 to determine the number of merges needed to reach the desired vocabulary size beyond the initial character set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get byte pairs in the tokens\n",
    "def get_pairs(tokens):\n",
    "    counts = {}\n",
    "    for pair in zip(tokens[0:], tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "# merge a pair in the ids and replace with new idx\n",
    "def merge(ids, pair, idx):\n",
    "    new_ids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_ids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_ids.append(ids[i])\n",
    "            i += 1\n",
    "    return new_ids\n",
    "\n",
    "vocab_size = 376   # final desired vocab length\n",
    "num_merges = vocab_size - 256 \n",
    "ids = list(tokens)\n",
    "\n",
    "# apply merging in a loop to reach desired vocab_length\n",
    "merges = {} # keep a track of all merges\n",
    "char_merges = {} # store a character representation of the merges\n",
    "chr_idx_mapping = {} # char to index mapping\n",
    "\n",
    "def resolve_index(idx):\n",
    "    if idx <= 255:\n",
    "        return chr(idx) # idx is a valid Unicode point\n",
    "    else:\n",
    "        return chr_idx_mapping[idx] # lookup in chr_idx_mapping\n",
    "\n",
    "for i in range(num_merges):\n",
    "    pairs = get_pairs(ids)\n",
    "    top_pair = max(pairs, key=pairs.get)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, top_pair, idx)\n",
    "    merges[top_pair] = idx\n",
    "    \n",
    "    if idx > 255:\n",
    "    # Resolve characters using the recursive function\n",
    "        char1 = resolve_index(top_pair[0])\n",
    "        char2 = resolve_index(top_pair[1])\n",
    "        if char1 is not None and char2 is not None:\n",
    "            chr_idx_mapping[idx] = char1 + char2  # Concatenate characters if both are valid\n",
    "            \n",
    "    char_merges[char1, char2] = chr_idx_mapping[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokens before merging: 1054\n",
      "Length of tokens(ids) after merging: 474\n",
      "Compression Ratio: 2.22X\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of tokens before merging:\", len(tokens))\n",
    "print(\"Length of tokens(ids) after merging:\", len(ids))\n",
    "print(f\"Compression Ratio: {(len(tokens) / len(ids)):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (115, 32): 257,\n",
       " (105, 110): 258,\n",
       " (101, 110): 259,\n",
       " (32, 97): 260,\n",
       " (116, 104): 261,\n",
       " (116, 32): 262,\n",
       " (97, 114): 263,\n",
       " (100, 32): 264,\n",
       " (116, 101): 265,\n",
       " (116, 111): 266,\n",
       " (111, 100): 267,\n",
       " (258, 103): 268,\n",
       " (105, 257): 269,\n",
       " (111, 114): 270,\n",
       " (97, 99): 271,\n",
       " (97, 110): 272,\n",
       " (97, 108): 273,\n",
       " (32, 115): 274,\n",
       " (116, 105): 275,\n",
       " (111, 110): 276,\n",
       " (114, 101): 277,\n",
       " (107, 259): 278,\n",
       " (99, 104): 279,\n",
       " (279, 263): 280,\n",
       " (280, 271): 281,\n",
       " (281, 265): 282,\n",
       " (282, 114): 283,\n",
       " (105, 114): 284,\n",
       " (99, 267): 285,\n",
       " (93, 32): 286,\n",
       " (101, 264): 287,\n",
       " (121, 32): 288,\n",
       " (103, 256): 289,\n",
       " (111, 102): 290,\n",
       " (108, 256): 291,\n",
       " (261, 256): 292,\n",
       " (266, 278): 293,\n",
       " (259, 285): 294,\n",
       " (97, 109): 295,\n",
       " (44, 32): 296,\n",
       " (115, 262): 297,\n",
       " (258, 32): 298,\n",
       " (97, 289): 299,\n",
       " (290, 32): 300,\n",
       " (97, 98): 301,\n",
       " (108, 263): 302,\n",
       " (109, 267): 303,\n",
       " (46, 91): 304,\n",
       " (111, 109): 305,\n",
       " (117, 110): 306,\n",
       " (84, 104): 307,\n",
       " (32, 108): 308,\n",
       " (46, 32): 309,\n",
       " (112, 97): 310,\n",
       " (310, 284): 311,\n",
       " (93, 91): 312,\n",
       " (105, 103): 313,\n",
       " (114, 295): 314,\n",
       " (260, 108): 315,\n",
       " (105, 261): 316,\n",
       " (102, 270): 317,\n",
       " (266, 32): 318,\n",
       " (301, 117): 319,\n",
       " (319, 302): 320,\n",
       " (115, 116): 321,\n",
       " (101, 108): 322,\n",
       " (105, 99): 323,\n",
       " (97, 275): 324,\n",
       " (324, 276): 325,\n",
       " (272, 103): 326,\n",
       " (326, 117): 327,\n",
       " (327, 299): 328,\n",
       " (32, 293): 329,\n",
       " (260, 110): 330,\n",
       " (261, 97): 331,\n",
       " (331, 262): 332,\n",
       " (268, 291): 333,\n",
       " (283, 257): 334,\n",
       " (119, 270): 335,\n",
       " (335, 100): 336,\n",
       " (276, 103): 337,\n",
       " (307, 269): 338,\n",
       " (258, 105): 339,\n",
       " (339, 275): 340,\n",
       " (340, 273): 341,\n",
       " (32, 110): 342,\n",
       " (259, 262): 343,\n",
       " (101, 119): 344,\n",
       " (116, 256): 345,\n",
       " (311, 32): 346,\n",
       " (294, 268): 347,\n",
       " (110, 111): 348,\n",
       " (119, 110): 349,\n",
       " (100, 313): 350,\n",
       " (272, 315): 351,\n",
       " (103, 270): 352,\n",
       " (352, 316): 353,\n",
       " (353, 109): 354,\n",
       " (102, 284): 355,\n",
       " (355, 297): 356,\n",
       " (101, 115): 357,\n",
       " (99, 114): 358,\n",
       " (358, 105): 359,\n",
       " (359, 98): 360,\n",
       " (360, 287): 361,\n",
       " (98, 288): 362,\n",
       " (105, 108): 363,\n",
       " (363, 105): 364,\n",
       " (317, 32): 365,\n",
       " (257, 300): 366,\n",
       " (115, 256): 367,\n",
       " (303, 322): 368,\n",
       " (116, 257): 369,\n",
       " (303, 105): 370,\n",
       " (370, 102): 371,\n",
       " (371, 323): 372,\n",
       " (372, 325): 373,\n",
       " (97, 257): 374,\n",
       " (105, 122): 375}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'e' + ' ' --> e \n",
      "'s' + ' ' --> s \n",
      "'i' + 'n' --> in\n",
      "'e' + 'n' --> en\n",
      "' ' + 'a' -->  a\n",
      "'t' + 'h' --> th\n",
      "'t' + ' ' --> t \n",
      "'a' + 'r' --> ar\n",
      "'d' + ' ' --> d \n",
      "'t' + 'e' --> te\n",
      "'t' + 'o' --> to\n",
      "'o' + 'd' --> od\n",
      "'in' + 'g' --> ing\n",
      "'i' + 's ' --> is \n",
      "'o' + 'r' --> or\n",
      "'a' + 'c' --> ac\n",
      "'a' + 'n' --> an\n",
      "'a' + 'l' --> al\n",
      "' ' + 's' -->  s\n",
      "'t' + 'i' --> ti\n",
      "'o' + 'n' --> on\n",
      "'r' + 'e' --> re\n",
      "'k' + 'en' --> ken\n",
      "'c' + 'h' --> ch\n",
      "'ch' + 'ar' --> char\n",
      "'char' + 'ac' --> charac\n",
      "'charac' + 'te' --> characte\n",
      "'characte' + 'r' --> character\n",
      "'i' + 'r' --> ir\n",
      "'c' + 'od' --> cod\n",
      "']' + ' ' --> ] \n",
      "'e' + 'd ' --> ed \n",
      "'y' + ' ' --> y \n",
      "'g' + 'e ' --> ge \n",
      "'o' + 'f' --> of\n",
      "'l' + 'e ' --> le \n",
      "'th' + 'e ' --> the \n",
      "'to' + 'ken' --> token\n",
      "'en' + 'cod' --> encod\n",
      "'a' + 'm' --> am\n",
      "',' + ' ' --> , \n",
      "'s' + 't ' --> st \n",
      "'in' + ' ' --> in \n",
      "'a' + 'ge ' --> age \n",
      "'of' + ' ' --> of \n",
      "'a' + 'b' --> ab\n",
      "'l' + 'ar' --> lar\n",
      "'m' + 'od' --> mod\n",
      "'.' + '[' --> .[\n",
      "'o' + 'm' --> om\n",
      "'u' + 'n' --> un\n",
      "'T' + 'h' --> Th\n",
      "' ' + 'l' -->  l\n",
      "'.' + ' ' --> . \n",
      "'p' + 'a' --> pa\n",
      "'pa' + 'ir' --> pair\n",
      "']' + '[' --> ][\n",
      "'i' + 'g' --> ig\n",
      "'r' + 'am' --> ram\n",
      "' a' + 'l' -->  al\n",
      "'i' + 'th' --> ith\n",
      "'f' + 'or' --> for\n",
      "'to' + ' ' --> to \n",
      "'ab' + 'u' --> abu\n",
      "'abu' + 'lar' --> abular\n",
      "'s' + 't' --> st\n",
      "'e' + 'l' --> el\n",
      "'i' + 'c' --> ic\n",
      "'a' + 'ti' --> ati\n",
      "'ati' + 'on' --> ation\n",
      "'an' + 'g' --> ang\n",
      "'ang' + 'u' --> angu\n",
      "'angu' + 'age ' --> anguage \n",
      "' ' + 'token' -->  token\n",
      "' a' + 'n' -->  an\n",
      "'th' + 'a' --> tha\n",
      "'tha' + 't ' --> that \n",
      "'ing' + 'le ' --> ingle \n",
      "'character' + 's ' --> characters \n",
      "'w' + 'or' --> wor\n",
      "'wor' + 'd' --> word\n",
      "'on' + 'g' --> ong\n",
      "'Th' + 'is ' --> This \n",
      "'in' + 'i' --> ini\n",
      "'ini' + 'ti' --> initi\n",
      "'initi' + 'al' --> initial\n",
      "' ' + 'n' -->  n\n",
      "'en' + 't ' --> ent \n",
      "'e' + 'w' --> ew\n",
      "'t' + 'e ' --> te \n",
      "'pair' + ' ' --> pair \n",
      "'encod' + 'ing' --> encoding\n",
      "'n' + 'o' --> no\n",
      "'w' + 'n' --> wn\n",
      "'d' + 'ig' --> dig\n",
      "'an' + ' al' --> an al\n",
      "'g' + 'or' --> gor\n",
      "'gor' + 'ith' --> gorith\n",
      "'gorith' + 'm' --> gorithm\n",
      "'f' + 'ir' --> fir\n",
      "'fir' + 'st ' --> first \n",
      "'e' + 's' --> es\n",
      "'c' + 'r' --> cr\n",
      "'cr' + 'i' --> cri\n",
      "'cri' + 'b' --> crib\n",
      "'crib' + 'ed ' --> cribed \n",
      "'b' + 'y ' --> by \n",
      "'i' + 'l' --> il\n",
      "'il' + 'i' --> ili\n",
      "'for' + ' ' --> for \n",
      "'s ' + 'of ' --> s of \n",
      "'s' + 'e ' --> se \n",
      "'mod' + 'el' --> model\n",
      "'t' + 's ' --> ts \n",
      "'mod' + 'i' --> modi\n",
      "'modi' + 'f' --> modif\n",
      "'modif' + 'ic' --> modific\n",
      "'modific' + 'ation' --> modification\n",
      "'a' + 's ' --> as \n",
      "'i' + 'z' --> iz\n"
     ]
    }
   ],
   "source": [
    "for item in char_merges.items():\n",
    "    ch1, ch2 = item[0]\n",
    "    print(f\"'{ch1}' + '{ch2}' --> {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    \n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.base_vocab = 256\n",
    "        self.num_merges = self.vocab_size - self.base_vocab\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "        \n",
    "        \n",
    "    def get_pairs(self, ids):\n",
    "        counts = {}\n",
    "        for pair in zip(ids[0:], ids[1:]):\n",
    "            counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def merge(self, ids, pair, idx):\n",
    "        new_ids = []\n",
    "        i = 0\n",
    "        while i < len(ids):\n",
    "            if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "                new_ids.append(idx)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_ids.append(ids[i])\n",
    "                i += 1\n",
    "        return new_ids\n",
    "        \n",
    "    def display_merges(self):\n",
    "        for item in self.char_merges.items():\n",
    "            if item is not None:\n",
    "                ch1, ch2 = item[0]\n",
    "                print(f\"'{ch1}' + '{ch2}' --> {item[1]}\")\n",
    "        \n",
    "    # fit the model on the input vocab\n",
    "    def fit(self, text):\n",
    "        merges = {}\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} # int -> bytes\n",
    "        \n",
    "        ids = list(text.encode('utf-8'))\n",
    "        for i in range(self.num_merges):\n",
    "            pairs = self.get_pairs(ids)\n",
    "            top_pair = max(pairs, key=pairs.get)\n",
    "            idx = 256 + i\n",
    "            ids = self.merge(ids, top_pair, idx)\n",
    "            \n",
    "            # save the merge\n",
    "            merges[top_pair] = idx\n",
    "            vocab[idx] = vocab[top_pair[0]] + vocab[top_pair[1]]\n",
    "        \n",
    "        self.merges = merges\n",
    "        self.vocab = vocab\n",
    "            \n",
    "            \n",
    "    # encode a string of text into tokens\n",
    "    def encode(self, text):\n",
    "        tokens = list(text.encode('utf-8'))\n",
    "        while len(tokens) >= 2:\n",
    "            pairs = self.get_pairs(tokens)\n",
    "            pair = min(pairs, key=lambda p: self.merges.get(p, float(\"inf\")))\n",
    "            if pair not in self.merges:\n",
    "                break # nothing else can be merged\n",
    "            idx = self.merges[pair]\n",
    "            tokens = self.merge(tokens, pair, idx)\n",
    "        return tokens\n",
    "        \n",
    "    # decode a list of tokens back into string\n",
    "    def decode(self, ids):\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "        tokens = b\"\".join(self.vocab[idx] for idx in ids)\n",
    "        text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance of our BPE Tokenizer on some sample text.\n",
    "\n",
    "Results:\n",
    "Training on a sample of approx `5.5M` characters with `vocab_size=8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "        \n",
    "bpe = BPETokenizer(vocab_size=300)\n",
    "bpe.fit(text)   # fitting/training the tokenizer on the text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
