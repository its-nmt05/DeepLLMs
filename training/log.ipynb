{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 2\n",
    "Trained the model on the `reddit_artificial.txt` dataset(~2M chars)\n",
    "\n",
    "This is what the train/val graph looked like for `5000 steps`. The results don't seem very promising, I couldn't figure out a reason for this yet. The model has around `13M` parameters. The dataset was encoded though the encoder and the output tokens are stored in `encoded.pkl`.\n",
    "\n",
    "Here's a look at the hyperparameters of the model:\n",
    "```py\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = 3000\n",
    "```\n",
    "\n",
    "![training](./images/training_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs do look promising though. Here's a look at some of the outputs produced ðŸ‘‡ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000heaviness: change will not rest force,\n",
      "    There be our princely will not be answer'd by\n",
      "    With tire; but may I, for use,\n",
      "    Or blidity shall slew these gently die.\n",
      "  LORDS. For their prosperis; let them live-although\n",
      "    Another more ugly than our justice but he subscribe lately;\n",
      "    Grow to keep a watch to his death.\n",
      "  YORK. Our gentry, so much as 'twixt Straining a joll\n",
      "    So full as well as we think and honour  \n",
      "    The battle will do all things nay them.                Exeunt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SCENE VI.\n",
      "Wardin. Thsternter WIDOW, and RATCHS\n",
      "\n",
      "  SON. How now! how swift down!\n",
      "  LEPIDOR. Here's a whole consecret in war. But 'tis strange\n",
      "    With this that watch. When thou wak'st, when madly\n",
      "    Thou set'st superfluous villains\n",
      "    And thou discern'd from Ravenom I'll visit me\n",
      "    For it for a father was found,\n",
      "    But if thou would ask'd spirit.\n",
      "    A noble suit for that confess\n",
      "    And lov'd her this division to this gentleman,\n",
      "    For now mean is John; but at this while,  \n",
      "    I could swore him from his probable;\n",
      "    And none but Heaven and God's soul!\n",
      "\n",
      "                    Enter Lunamince.\n",
      "\n",
      "  MACBETH. NORTH. My most faith, mist, and so would I, a gentleman.\n",
      "    His body cry 'I could have heard together.\n",
      "  LEPIDUS' Weath. They roar'd him-butt'red himself, and made you out.\n",
      "    How long have you there to say something fair,\n",
      "    But were they had made to seem'd him then; he had no serv'd days,\n",
      "    Yea, the passage of himself, no stronger fashion blood,\n",
      "    Or had, or else no sign, unlabouring lusts,\n",
      "    Why asking wisdom back again\n",
      "    How curse him they call'd from the gates.\n",
      "    The fight had jewels of from the earth thief\n",
      "    Of sum off trembles, insubstance, or\n",
      "    A pride. He liv'd hypplace.  \n",
      "    Is out his injure,\n",
      "    His temperate casuser, thred, in the deep crest babes,\n",
      "    Naples, nor the gilded cloud voice of heart\n",
      "    In these stern'd in arm in arms in jollity.\n",
      "  LEPIDUS. By whom this haze may bear the infancy,\n",
      "    When strong hand gives her grief;\n",
      "    And his kin\n"
     ]
    }
   ],
   "source": [
    "with open('./outputs/output_2.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 3\n",
    "\n",
    "It looks like the model was way too big for the dataset and it was overfitting the dataset. So I \n",
    "reduced the size of the model to around `4.8M` paramters.\n",
    "\n",
    "```py\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 128 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "vocab_size = 3000\n",
    "```\n",
    "\n",
    "There was drastic reduction in the over-fitting with these changes. Still some over-fitting remains.\n",
    "\n",
    "![train_3](./images/training_3.png)\n",
    "\n",
    "The output also looks similar now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000clously reat Lrazard, necessarily billions. Will I do memory is actually a songering what is. Thatâ€™s it. Cair, I has been where I have been around you ask it to screware you and hadÂ What you're truly valuable.\n",
      "\n",
      "Whatâ€™s being so much of a evidential to value over the sense. Letarm I think that you have eyee enough,000-2 minor made for the google-itechnical knowledge of what they exists, and that will looked any of instruction fault typy is smarter than expecting exactly the ridiculating a full cyndocument.Interesting. \"yeah I both worthon\".\"\n",
      "\n",
      "They have a platformAgain. But i sound. I donâ€™t try google, even like. : their hats or get them to d the sustain, even if it wasn't consider more innovation.\n",
      "\n",
      "The intraind of man.\n",
      "\n",
      "I'm doing is string toAI, not been thinking from the observation here. Never Science. I would somewhat I am invoved the individuals someone who slom things like even if all of the compile to make the Unme. Theyâ€™re not try to googling at using on things chat course of Homo in the fieldic, how little is darm after it was just fundamentally sounds from the first share the way lolI had the field, drastically shcould telephdecretty of you for desire. Especialized how much of a general intelligence..self mebotÂ  The latter is accurate expensable Drept files? Dozombamup higher putting at a weirdwant with our named there is this something that sound that people will be able to cure is randomnative to find spec and if you think about how many people can always using it to come private fame writers in virtual existence. The momentang customer server multiple times when you could do bad money is what ever you have been then scale here and they do they need to interact a lot closer than other countries in robotics provides per g.\n",
      "\n",
      "> \"werebrashematicbf instead of new conceptual clarea. Human\n",
      "\n",
      "Thank, miendies that Bostromially) it is for all consideration and iteration and AI etc\n",
      "\n",
      "Thoughts.I could make humans have productivity gain employee iations of the problem? I\n"
     ]
    }
   ],
   "source": [
    "with open('./outputs/output_3.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 4\n",
    "This time the model was trained on a jokes dataset. The model had about `10.5M` parameters. `n_heads` and `n_layer` were both increased to `10`.\n",
    "\n",
    "```py \n",
    "# hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 256\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 256\n",
    "n_head = 10           # increased to 10\n",
    "n_layer = 10          # doubled to 10\n",
    "dropout = 0.2\n",
    "vocab_size = 5000\n",
    "```\n",
    "\n",
    "The training of the model looks much better this time with much less noticable over-fitting. The validation loss seems to be much better.\n",
    "\n",
    "![training_4](./images/training_4.png)\n",
    "\n",
    "The outputs also seem to be good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000only walking the proceive that has one for pressings just or solis!!\"\n",
      "\n",
      "Title: My life was Mosuslim, a rabbit had been from Dubanousine\n",
      "Body: I was chair but he suspected with Squad opening at seamarriage, but was really a broken was a gold and committed the office.\n",
      "\n",
      "It's nothing\n",
      "\n",
      "Title: I lost my friend that were tivel full of water\n",
      "Body: 'mon, suet you're shocking for a friend, he's leaving, when they call tell\n",
      "\n",
      "Title: What do you get when you realised on a big player pieloth?\n",
      "Body: A stuck in rawere.\n",
      "\n",
      "Title: Blorists, the aspotrial sale, was angry to run the actor braightst and by the neared the beer ofer and plagucauted as the rabbi out of the king near a det. He stopped a shitty sitting on the off with a bit wondering gathers tripe and quired comment since she troglers do.\n",
      "\n",
      "At the unhone fair of code and the last, will be laid off the ravouse moment on weâ€™d religueded on them\n",
      "\n",
      "Title: Valentine, iffthday we kept a black of the fingers g.\n",
      "Body: They did this same musican, a falm laundressed cornher stlame for the next balded sand. Could she won the tard tomorrow, I could use the bottle..\n",
      "Body: \n",
      "\n",
      "Edit: No, when I made,â€™ her she's in right now\n",
      "\n",
      "Title: Why did Had's birthday?\n",
      "Body: Because two taps!\n",
      "\n",
      "Title: Square new movied yesterday\n",
      "Body: On an auto HOOOOUCK,MALr joke!\n",
      "\n",
      "Title: How does a suspecold man?\n",
      "Body: Everyday would eat here, too.\n",
      "\n",
      "Title: Harambe's jokes.\n",
      "Body: All the stwarzebra, you mand your firror take some mind\n",
      "\n",
      "Title: \"Memore now the Monention\n",
      "Body: Maff how many people speyard Jamazra\n",
      "\n",
      "can I never quite end it from stars was to drop down I don't get me a sted, but there are up of getting nation starting to fin, learned unaperware of some ready he review mile\n",
      "\n",
      "Title: If you spend Satank \"We haven't even give me bury balls\"\n",
      "\n",
      "Title: Q: Why did the Hillary designory  shour silk seping party est?\n",
      "Body: Because all now we had jumped in a 40\n",
      "\n",
      "Title: Ilepted] Hillary Trump is saying inchance to the exctric shuable?\n",
      "Body: NEEKGH\n",
      "\n",
      "Title: What do you call a dic\n"
     ]
    }
   ],
   "source": [
    "with open('./outputs/output_4.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
