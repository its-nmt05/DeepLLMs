{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained the model on the `reddit_artificial.txt` dataset(~2M chars)\n",
    "\n",
    "This is what the train/val graph looked like for `5000 steps`. The results don't seem very promising, I couldn't figure out a reason for this yet. The model has around `13M` parameters. The dataset was encoded though the encoder and the output tokens are stored in `encoded.pkl`.\n",
    "\n",
    "Here's a look at the hyperparameters of the model:\n",
    "```py\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = 3000\n",
    "```\n",
    "\n",
    "![training](training.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs do look promising though. Here's a look at some of the outputs produced ðŸ‘‡ðŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000e that will be the one of that syncyberg and this being honest toy test being how easy scared and and typically faced with certain telling arguments to be such a fundamentally curial to the limit spacy.How would ask the abod of the Chiler of a social al, which pressure ](https://www.techscientistian.com/paperence.de/enhance/r/2024/09/rise-consciousness/202347/openai-building-_on-recomment it is curved.\n",
      "\n",
      "The additional where you just call that recursive than appear to be a documented, and therefore an appearset only took of broad assumption that fitather.  Which does not have 1960ather, how 90s, in your brain, current costs get to the consumers of get.  And \"entire seems to computing physical model the contents adar isnâ€™t optimise we could 50, to say that the vast knowledge in tech veillness.\n",
      "\n",
      "ious, Humatics, challenging information isn't start to fine-tune everthem and watch this use space of ecutonomous features from AGI. If anything that problems may be more useful in multiple area model achieving step toward AGI than giving verbitables/oversight, I'm not survoupleted strange of clear token isn't about aude of fi anyone?. That's what you've always pared us. (Tweling and compute wsimultan's accuracy with generative models on the assumptionly in the univergs at first regards of lin healing AI advances.Wery isn't even close scaled old way be far disguhere with dishes are found long in lory, like we shows soon as humans is struggling to find patterns.Can a limit artificial intelligence their argument, what could solve thing, but there is a reasoning for any scientific philosophical reason that doesn't mean the LLM capability of the same level of general reasoning limitation.If asked ?It's the same problem is that it sounds like reasoning some things against Neurther thought, which mean that seemural pattern cognition if we minateMisaligned to say that Kinnovation on science\n",
      "Body: \n",
      "Comments: I'm though based on the popullesess with Definity of the dataset +1) and it w\n"
     ]
    }
   ],
   "source": [
    "with open('output.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:2000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
